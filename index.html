<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jinbao Wang (ÁéãÈáëÂÆù)</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üéà</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>PhD Jinbao Wang</name>
              </p>
              <p><a href="https://bdsc.szu.edu.cn/info/1179/1707.htm">Assistant Professor</a>, School of Artificial Intelligence, 
                <a href="https://bdsc.szu.edu.cn/">National Engineering Laboratory for Big Data System Computing Technology</a>, 
                <a href="https://www.szu.edu.cn">Shenzhen University (SZU)</a>. I work on digital human modelling and driving, image anomaly detection, computer vision and machine learning.
              </p>
              <p style="text-align:center">
                <a href="data/Jinbao_Wang_CV.pdf">CV</a> &nbsp/&nbsp
                <a href="data/Jinbao_Wang_CV_CH.pdf">ÁÆÄÂéÜ</a> &nbsp/&nbsp
                <a href="data/Jinbao_Wang_Bio.txt">Bio</a> &nbsp/&nbsp
                <a href="https://github.com/jinbao-wang">Github</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=qI80ipUAAAAJ">Google Scholar</a> &nbsp&nbsp
                <!-- <a href="mailto:linkingring@163.com">linkingring@163.com</a>  -->
              </p>
              <p>
                If you have any questions or wish to cooperate with us, please feel free to contact me. 
                Email: <a href="mailto:wangjb@szu.edu.cn">wangjb@szu.edu.cn</a> / <a href="mailto:wangjb@ieee.org">wangjb@ieee.org</a>
              </p>
            </td>
            <td style="padding:2.5%;width:27%;max-width:27%">
              <a href="images/Jinbao-Wang.jpg"><img style="width:80%;max-width:80%" alt="profile photo" src="images/Jinbao-Wang.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading> <b>Research</b></heading>
              <p>
                I'm interested in image anomaly detection, graph representation learning, computer vision, and machine learning.
                Much of my research is about detecting and localizing anomalies from images. Representative papers are <span class="highlight">highlighted</span>.
              </p>
              <p>Note that *contributed equally, ‚Ä†corresponding author</p>
            </td>
          </tr>
        </tbody> </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

  <!-- Image/3D Anomaly Detection -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
      <p><b> üî• &nbsp&nbsp&nbsp&nbsp&nbsp 3D/Image Anomaly Detection </b></p> 
      </td>
    </tr>


    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/iadsurvey.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://link.springer.com/article/10.1007/s11633-023-1459-z">
          <papertitle>Deep Industrial Image Anomaly Detection: A Survey üìç</papertitle>
        </a>
        <br>
        Jiaqi Liu*, Guoyang Xie*, <strong>Jinbao Wang</strong>*, Shangnian Li, Chengjie Wang, Feng Zheng, Yaochu Jin
        <br>
        <em>Machine Intelligence Research (MIR)</em>, 2024.
        <br>
        <a href="https://github.com/M-3LAB/awesome-industrial-anomaly-detection">project page</a>
        /
        <a href="https://link.springer.com/article/10.1007/s11633-023-1459-z">springer</a>
        /
        <a href="https://arxiv.org/pdf/2301.11514.pdf">arXiv</a>
        /
        <a href="https://link.springer.com/journal/11633">impact factor</a>
        <p></p>
        <p>
          We provide a comprehensive review of deep learning-based IAD from the perspectives of neural network architectures, levels of supervision, loss functions, metrics and datasets. 
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/imiad.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2301.13359.pdf">
          <papertitle>IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing üìç</papertitle>
        </a>
        <br>
        Guoyang Xie*, <strong>Jinbao Wang</strong>*, Jiaqi Liu*, Jiayi Lyu, Yong Liu, Chengjie Wang, Feng Zheng, Yaochu Jin
        <br>
        <em>IEEE Transactions on Cybernetics (IEEE TCYB)</em>, 2024. &nbsp <font color="red">(‰∏≠ÁßëÈô¢Â§ßÁ±ª1Âå∫)</font> &nbsp
        <br>
        <a href="https://github.com/M-3LAB/open-iad">project page</a>
        /
        <a href="https://arxiv.org/pdf/2301.13359.pdf">arXiv</a>
        /
        <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6221036">impact factor</a>
        <p></p>
        <p>
          We propose a large-scale systematic benchmark and uniform setting for IAD to bridge the gap between academy and industrial manufacturing. 
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/dusnet.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="NeedUpload">
          <papertitle>Taming Anomalies with Down-Up Sampling Networks: Group Center Preserving Reconstruction for 3D Anomaly Detection</papertitle>
        </a>
        <br>
        Hanzhe Liang, Jie Zhang, Tao Dai, Linlin Shen, <strong>Jinbao Wang</strong>, Can Gao. 
        <br>
        <em>ACM Multimedia (ACM MM)</em>, 2025. &nbsp <font color="red">(CCF-A)</font> &nbsp
        <br>
        <a href="">project page</a>
        /
        <a href="https://openreview.net/forum?id=4UepaTrwZE&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3Dacmmm.org%2FACMMM%2F2025%2FConference%2FAuthors%23your-submissions)">openreview</a>
        /
        <a href="">arxiv</a>
        <p></p>
        <p>
          Reconstruction-based methods have demonstrated very promising results for 3D anomaly detection. However, these methods face great challenges in handling high-precision point clouds due to the large scale and complex structure. In this study, a Down-Up Sampling Networks (DUS-Net) is proposed to reconstruct high-precision point clouds for 3D anomaly detection by preserving the group center geometric structure. 
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/mc3d-ad.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2505.01969">
          <papertitle>MC3D-AD: A Unified Geometry-aware Reconstruction Model for Multi-category 3D Anomaly Detection</papertitle>
        </a>
        <br>
        Jiaqi Liu*, Guoyang Xie*, <strong>Jinbao Wang</strong>*, Shangnian Li, Chengjie Wang, Feng Zheng, Yaochu Jin
        <br>
        <em>The 34th International Joint Conference on Artificial Intelligence (IJCAI) </em>, 2025. &nbsp <font color="red">(CCF-A)</font> &nbsp
        <br>
        <a href="https://github.com/iCAN-SZU/MC3D-AD">project page</a>
        /
        <a href="https://arxiv.org/abs/2505.01969">arXiv</a>
        <p></p>
        <p>
         This paper presents a novel unified model for Multi-Category 3D Anomaly Detection (MC3D-AD) that aims to utilize both local and global geometry-aware information to reconstruct normal representations of all categories. 
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/lookinside.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2412.13461">
          <papertitle>Look Inside for More: Internal Spatial Modality Perception for 3D Anomaly Detection</papertitle>
        </a>
        <br>
        Hanzhe Liang, Guoyang Xie, Chengbin Hou, Bingshu Wang, Can Gao‚Ä†, and <strong>Jinbao Wang</strong>‚Ä†. 
        <br>
        <em>Association for the Advancement of Artificial Intelligence (AAAI)</em>, 2025. &nbsp <font color="red">(CCF-A)</font> &nbsp
        <br>
        <a href="https://github.com/M-3LAB/looking-inside-for-more">project page</a>
        /
        <a href="https://openreview.net/forum?id=fHJLvFx7pE&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DAAAI.org%2F2025%2FConference%2FAuthors%23your-submissions)">openreview</a>
        /
        <a href="https://arxiv.org/abs/2412.13461">arxiv</a>
        <p></p>
        <p>
          To amplify spatial structure feature representation, it is the first effort to leverage the internal information embedded within samples. Inspired by the basic intuition of why not look inside for more, we observed this prototype is straightforward and effective. As a result, we introduce a newly designed mode named Internal Spatial Modality Perception (ISMP) to explore the feature representation from internal views fully. 
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/group3ad.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="NeedUpload">
          <papertitle>Towards High-resolution 3D Anomaly Detection via Group-Level Feature Contrastive Learning</papertitle>
        </a>
        <br>
        Hongze Zhu, Guoyang Xie, Chengbin Hou, Tao Dai, Can Gao, <strong>Jinbao Wang</strong>‚Ä†, Linlin Shen. 
        <br>
        <em>ACM Multimedia (ACM MM)</em>, 2024. &nbsp <font color="red">(CCF-A)</font> &nbsp
        <br>
        <a href="https://github.com/M-3LAB/Group3AD">project page</a>
        /
        <a href="https://openreview.net/forum?id=XRMvayh6Vk&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3Dacmmm.org%2FACMMM%2F2024%2FConference%2FAuthors%23your-submissions)">openreview</a>
        /
        <a href="https://arxiv.org/abs/2408.04604">arxiv</a>
        <p></p>
        <p>
          To achieve better representation, we pose a key question: how to create an ideal distribution required by HRPCD-AD in the feature space? This paper propose a novel group-level feature-based network, called Group3AD, which has a significantly efficient representation ability. The experimental result surpasses Reg3D-AD by the margin of 5% in terms of object-level AUROC on Real3D-AD.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/ucad.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="NeedUpload">
          <papertitle>Unsupervised Continual Anomaly Detection with Contrastively-learned Prompt</papertitle>
        </a>
        <br>
        Jiaqi Liu, Kai Wu, Qiang Nie, Ying Chen, Bin-Bin Gao, Yong Liu, <strong>Jinbao Wang</strong>, Chengjie Wang, Feng Zheng. 
        <br>
        <em> Association for the Advancement of Artificial Intelligence (AAAI) </em>, 2024. &nbsp <font color="red">(CCF-A)</font> &nbsp 
        <br>
        <a href="https://github.com/shirowalker/UCAD">project page</a>
        /
        <a href="https://arxiv.org/abs/2401.01010">arxiv</a>
        <p></p>
        <p>
          This project introduces a novel Unsupervised Continual Anomaly Detection framework (UCAD), which equips the unsupervised AD with continual learning capability through contrastively-learned prompts.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/real3dad.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2309.13226.pdf">
          <papertitle>Real3D-AD: A Dataset of Point Cloud Anomaly Detection</papertitle>
        </a>
        <br>
        Jiaqi Liu, Guoyang Xie, Ruitao Chen, Xinpeng Li, <strong>Jinbao Wang</strong>‚Ä†, Yong Liu, Chengjie Wang, Feng Zheng‚Ä†
        <br>
        <em> NeurIPS Datasets & Benchmarks Track </em>, 2023. &nbsp <font color="red">(CCF-A)</font> &nbsp
        <br>
        <a href="https://github.com/M-3LAB/Real3D-AD">project page</a>
        /
        <a href="https://arxiv.org/pdf/2309.13226.pdf">openreview</a>
        <p></p>
        <p>
          This project aims to construct a new dataset of high-resolution 3D point clouds for anomaly detection tasks in real-world scenes. Real3D-AD comprises a total of 1,254 samples that are distributed across 12 distinct categories.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/easynet.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2307.13925.pdf">
          <papertitle>EasyNet: An Easy Network for 3D Industrial Anomaly Detection</papertitle>
        </a>
        <br>
        Ruitao Chen, Guoyang Xie, Jiaqi Liu, <strong>Jinbao Wang</strong>‚Ä†, Ziqi Luo, Jinfan Wang, Feng Zheng‚Ä†
        <br>
        <em>ACM Multimedia (ACM MM)</em>, 2023. &nbsp <font color="red">(CCF-A)</font> &nbsp
        <br>
        <a href="">project page</a>
        /
        <a href="https://arxiv.org/pdf/2307.13925.pdf">arXiv</a>
        <p></p>
        <p>
          This paper addresses a promising and challenging task, i.e., deployment-friendly 3D-AD and proposes an easy but effective neural network (EasyNet) to achieve competitive performance without using large pre-trained models and memory banks.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/graphcore.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2301.12082.pdf">
          <papertitle>Pushing the Limits of Fewshot Anomaly Detection in Industry Vision: Graphcore</papertitle>
        </a>
        <br>
        Guoyang Xie*, <strong>Jinbao Wang</strong>*‚Ä†, Jiaqi Liu*, Feng Zheng‚Ä†, Yaochu Jin
        <br>
        <em>International Conference on Learning Representations (ICLR)</em>, 2023.
        <br>
        <a href="https://iclr.cc/virtual/2023/poster/12228">iclr page</a>
        /
        <a href="https://openreview.net/pdf?id=xzmqxHdZAwO">openreview</a>
        /
        <a href="https://arxiv.org/pdf/2301.12082.pdf">arXiv</a>
        <p></p>
        <p>
          We reveal that rotation-invariant feature property has a significant impact in industrial-based fewshot anomaly detection.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/cvprw.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2304.03294.pdf">
          <papertitle>What Makes a Good Data Augmentation for Few-Shot Unsupervised Image Anomaly Detection</papertitle>
        </a>
        <br>
        Lingrui Zhang, Shuheng Zhang, Guoyang Xie, Jiaqi Liu, Hua Yan, <strong>Jinbao Wang</strong>‚Ä†, Feng Zheng‚Ä†, Yaochu Jin
        <br>
        <em>CVPR VISION Workshop (CVPRW)</em>, 2023.
        <br>
        <a href="https://openaccess.thecvf.com/content/CVPR2023W/VISION/papers/Zhang_What_Makes_a_Good_Data_Augmentation_for_Few-Shot_Unsupervised_Image_CVPRW_2023_paper.pdf">openaccess</a>
        /
        <a href="https://arxiv.org/pdf/2304.03294.pdf">arXiv</a>
        <p></p>
        <p>
          We systematically investigate various data augmentation methods for few-shot IAD algorithms.  
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/softpatch.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://openreview.net/pdf?id=pIYYJflkhZ">
          <papertitle>SoftPatch: Unsupervised Anomaly Detection with Noisy Data</papertitle>
        </a>
        <br>
        Xi Jiang, Jianlin Liu, <strong>Jinbao Wang</strong>‚Ä†, Qiang Nie, W. U. Kai, Yong Liu, Chengjie Wang, Feng Zheng‚Ä†
        <br>
        <em>Neural Information Processing Systems (NeurIPS)</em>, 2022. &nbsp <font color="red">(CCF-A)</font> &nbsp
        <br>
        <a href="https://github.com/M-3LAB/open-iad">project page</a>
        /
        <a href="https://openreview.net/pdf?id=pIYYJflkhZ">openreview</a>
        <p></p>
        <p>
          We propose a memory-based unsupervised AD method, which efficiently denoises the data at the patch level.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/continualad.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2201.12589.pdf">
          <papertitle>Towards Continual Adaptation in Industrial Anomaly Detection</papertitle>
        </a>
        <br>
        Wujin Li, Jiawei Zhan, <strong>Jinbao Wang</strong>‚Ä†, Bizhong Xia, Bin-Bin Gao, Jun Liu, Chengjie Wang, Feng Zheng‚Ä†
        <br>
        <em>ACM Multimedia (ACM MM)</em>, 2022. &nbsp <font color="red">(CCF-A)</font> &nbsp
        <br>
        <a href="https://github.com/M-3LAB/open-iad">project page</a>
        /
        <a href="https://openreview.net/pdf?id=pIYYJflkhZ">openreview</a>
        <p></p>
        <p>
          We propose a unified framework by incorporating continual learning to achieve our newly designed task of continual anomaly detection.
        </p>
      </td>
    </tr>

  <!-- Digital Human -->
  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
    <p><b> ü§π &nbsp&nbsp&nbsp&nbsp&nbsp Digital Human </b></p> 
    </td>
  </tr> 

  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/hairdiffusion.png' width="160">
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://neurips.cc/virtual/2024/poster/94970">
        <papertitle>HairDiffusion: Vivid Multi-Colored Hair Editing via Latent Diffusion</papertitle>
      </a>
      <br>
      Yu Zeng, Yang Zhang, Linlin Shen, Jiachen Liu, Kaijun Deng, Weizhao He, <strong>Jinbao Wang</strong>. 
      <br>
      <em>NeurIPS</em>, 2024. &nbsp <font color="red">(CCF-A)</font> &nbsp
      <br>
      <a href="">project page</a>
      /
      <a href="https://openreview.net/forum?id=UQflshLbZv&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DNeurIPS.cc%2F2024%2FConference%2FAuthors%23your-submissions)">openreview</a>
      /
      <a href="https://arxiv.org/html/2410.21789v1">arXiv</a>
      <p></p>
      <p>
        Hair editing is a critical image synthesis task that aims to edit hair color and hairstyle using text descriptions or reference images, while preserving irrelevant attributes (e.g., identity, background, cloth). This paper utilizes Latent Diffusion Models (LDMs) for hairstyle editing. The proposed method not only tackles the complexity of multi-color hairstyles but also addresses the challenge of preserving original colors during diffusion editing.
      </p>
    </td>
  </tr>

  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/posesurvey.png' width="160">
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://www.sciencedirect.com/science/article/pii/S1077314221000692">
        <papertitle>Deep 3D Human Pose Estimation: A Review</papertitle>
      </a>
      <br>
      <strong>Jinbao Wang</strong>*, Shujie Tan*, Xiantong Zhen, Shuo Xu, Feng Zheng, Zhenyu He, Ling Shao
      <br>
      <em>Computer Vision and Image Understanding (CVIU)</em>, 2021.
      <br>
      <a href="https://www.sciencedirect.com/science/article/pii/S1077314221000692">sciencedirect</a>
      /
      <a href="https://www.sciencedirect.com/journal/computer-vision-and-image-understanding">impact factor</a>
      <p></p>
      <p>
        The recent methods for deep 3D pose estimation are categorized and thoroughly analyzed. Provide an extensive review of related datasets and evaluation metrics. Compare the pros and cons of the deep 3D models valuated on the datasets and draw a conclusion.
      </p>
    </td>
  </tr>

  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/motioncapture.png' width="160">
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://ieeexplore.ieee.org/abstract/document/8683506">
        <papertitle>A Markerless Body Motion Capture System for Character Animation Based on Multi-view Cameras</papertitle>
      </a>
      <br>
      <strong>Jinbao Wang</strong>, Ke Lu, Jian Xue, Pengcheng Gao, Yanfu Yan
      <br>
      <em>IEEE International Conference on Acoustics, Speech and Signal Processing (IEEE ICASSP)</em>, 2019.
      <br>
      <a href="https://github.com/jinbao-wang/skinmesh">project page</a>
      /
      <a href="https://ieeexplore.ieee.org/abstract/document/8683506">ieeexplore</a>
      /
      <a href="https://arxiv.org/abs/2212.05788">arXiv</a> 
      <p></p>
      <p>
        A novel application system is proposed to achieve the generation of 3D character animation driven by markerless human body motion capture.
      </p>
    </td>
  </tr>

  <!-- Medical Federated Learning -->
    <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <p><b> üëë &nbsp&nbsp&nbsp&nbsp&nbsp Medical Federated Learning </b></p> 
    </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/medgansurvey.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2202.06997.pdf">
          <papertitle>Cross-Modality Neuroimage Synthesis: A Survey</papertitle>
        </a>
        <br>
        Guoyang Xie*, Yawen Huang*, <strong>Jinbao Wang</strong>‚Ä†, Jiayi Lyu, Feng Zheng‚Ä†, Yaochu Jin
        <br>
        <em>ACM Computing Surveys</em>, 2023. &nbsp <font color="red">(‰∏≠ÁßëÈô¢Â§ßÁ±ª1Âå∫)</font> &nbsp
        <br>
        <a href="https://dl.acm.org/doi/abs/10.1145/3625227">acmdl</a>
        /
        <a href="https://arxiv.org/pdf/2202.06997.pdf">arXiv</a>
        /
        <a href="https://dl.acm.org/journal/csur">impact factor</a>
        <p></p>
        <p>
          We provide a comprehensive review of cross-modality synthesis for neuroimages, from the perspectives of weakly-supervised and unsupervised settings, loss functions, evaluation metrics, ranges of modality, datasets, and the synthesis-based downstream applications.     
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/fedmedgan.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231223004058?via%3Dihub">
          <papertitle>FedMed-GAN: Misaligned Unpaired Brain Image Synthesis via Transform Loss</papertitle>
        </a>
        <br>
        <strong>Jinbao Wang</strong>*, Guoyang Xie*, Yawen Huang*, Jiayi Lyu, Feng Zheng, Yefeng Zheng, Yaochu Jin
        <br>
        <em>Neurocomputing</em>, 2023.
        <br>
        <a href="https://github.com/M-3LAB/FedMed-GAN">project page</a>
        /
        <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231223004058?via%3Dihub">sciencedirect</a>
        /
        <a href="https://arxiv.org/pdf/2201.08953.pdf">arXiv</a>
        /
        <a href="https://www.sciencedirect.com/journal/neurocomputing">impact factor</a>
        <p></p>
        <p>
          We propose a new benchmark for federated domain translation on unsupervised brain image synthesis to bridge the gap between federated learning and medical GAN.     
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/fedmedatl.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2201.12589.pdf">
          <papertitle>FedMed-ATL: Misaligned Unpaired Brain Image Synthesis via Transform Loss</papertitle>
        </a>
        <br>
        <strong>Jinbao Wang</strong>*, Guoyang Xie*, Yawen Huang*, Yefeng Zheng, Yaochu Jin, Feng Zheng
        <br>
        <em>ACM Multimedia (ACM MM)</em>, 2022. &nbsp <font color="red">(CCF-A)</font> &nbsp
        <br>
        <a href="https://github.com/M-3LAB/FedMed-GAN">project page</a>
        /
        <a href="https://arxiv.org/pdf/2201.12589.pdf">arXiv</a>
        <p></p>
        <p>
          We propose a method that can reduce the demands for deformable registration while encourage to leverage the misaligned and unpaired data. 
        </p>
      </td>
    </tr>

  <!-- Fast Retrieval -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
      <p><b> üê∞ &nbsp&nbsp&nbsp&nbsp&nbsp Fast Retrieval </b></p> 
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/alta.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/document/10347466">
          <papertitle>Cross-Modal Alternating Learning with Task-Aware Representations for Continual Learning
            </papertitle>
        </a>
        <br>
        Wujin Li, Bin-Bin Gao, Bizhong Xia, <strong>Jinbao Wang</strong>, Jun Liu, Yong Liu, Chengjie Wang, and Feng Zheng
        <br>
        <em> IEEE Transactions on Multimedia (IEEE TMM) </em>, 2023. &nbsp <font color="red">(‰∏≠ÁßëÈô¢Â§ßÁ±ª1Âå∫)</font> &nbsp
        <br>
        <a href="NeedUpload">project page</a>
        /
        <a href="https://ieeexplore.ieee.org/document/10347466">ieeexplore</a>
        /
        <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">impact factor</a>
        <p></p>
        <p>
          This paper proposes a novel yet effective framework coined cross-modal Alternating Learning with Task-Aware representations (ALTA) to make good use of visual and linguistic modal information and achieve more effective continual learning.
        </p>
      </td>
    </tr>
    
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/ccmh.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320323003631">
          <papertitle>Continuous Cross-Modal Hashing</papertitle>
        </a>
        <br>
        Hao Zheng*, <strong>Jinbao Wang</strong>*, Jinbao Wang, Xiantong Zhen, Jingkuan Song, Feng Zheng, Ke Lu, Guo-Jun Qi
        <br>
        <em> Pattern Recognition (PR) </em>, 2023. &nbsp <font color="red">(‰∏≠ÁßëÈô¢Â§ßÁ±ª1Âå∫)</font> &nbsp
        <br>
        <a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320323003631">sciencedirect</a>
        /
        <a href="https://www.sciencedirect.com/journal/pattern-recognition">imapct factor</a> 
        <p></p>
        <p>
          We propose a novel framework for the new task of continuous cross-modal hashing.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/tamnas.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2103.00363.pdf">
          <papertitle>Tiny Adversarial Multi-Objective Oneshot Neural Architecture Search</papertitle>
        </a>
        <br>
        Guoyang Xie*, <strong>Jinbao Wang</strong>*, Guo Yu, Jiayi Lyu, Feng Zheng, Yaochu Jin 
        <br>
        <em>Complex & Intelligent System (CIS)</em>, 2023.
        <br>
        <a href="https://link.springer.com/article/10.1007/s40747-023-01139-8">springer</a>
        /
        <a href="https://www.springer.com/journal/40747">impact factor</a>
        <p></p>
        <p>
          We propose a multi-objective oneshot network architecture search algorithm to obtain the best trade-off networks in terms of the adversarial accuracy, the clean accuracy and the model size.      
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/hgnn.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/9474952">
          <papertitle>Learning Efficient Hash Codes for Fast Graph-Based Data Similarity Retrieval</papertitle>
        </a>
        <br>
        <strong>Jinbao Wang</strong>, Shuo Xu, Feng Zheng, Ke Lu, Jingkuan Song, and Ling Shao 
        <br>
        <em>IEEE Transactions on Image Processing (IEEE TIP) </em>, 2021. &nbsp <font color="red">(CCF-A)</font> &nbsp &nbsp <font color="red">(‰∏≠ÁßëÈô¢Â§ßÁ±ª1Âå∫)</font> &nbsp
        <br>
        <a href="https://ieeexplore.ieee.org/abstract/document/9474952">ieeexplore</a>
        /
        <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">impact factor</a>
        <p></p>
        <p>
          To represent graph-based data, and maintain fast retrieval while doing so, we introduce an efficient hash model with graph neural networks for fast graph-based data retrieval.
        </p>
      </td>
    </tr>

  <!-- image propcessing -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
      <p><b> ‚òòÔ∏è &nbsp&nbsp&nbsp&nbsp&nbsp Image Propcessing </b></p> 
      </td>
    </tr>  

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/clickseg.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Chen_Seminar_Learning_for_Click-Level_Weakly_Supervised_Semantic_Segmentation_ICCV_2021_paper.html">
          <papertitle>Seminar Learning for Click-Level Weakly Supervised Semantic Segmentation</papertitle>
        </a>
        <br>
        Hongjun Chen, <strong>Jinbao Wang</strong>, Hong Cai Chen, Xiantong Zhen, Feng Zheng, Rongrong Ji, Ling Shao
        <br>
        <em>IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 2021. &nbsp <font color="red">(CCF-A)</font> &nbsp
        <br>
        <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Seminar_Learning_for_Click-Level_Weakly_Supervised_Semantic_Segmentation_ICCV_2021_paper.pdf">openaccess</a>
        /
        <a href="https://arxiv.org/abs/2108.13393">arXiv</a> 
        <p></p>
        <p>
          We propose seminar learning, a new learning paradigm for semantic segmentation with click-level supervision.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/dehazemsrcr.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/7984895">
          <papertitle>Single Image Dehazing Based on the Physical Model and MSRCR Algorithm</papertitle>
        </a>
        <br>
        <strong>Jinbao Wang</strong>, Ke Lu, Jian Xue, Ning He, Ling Shao
        <br>
        <em>IEEE Transactions on Circuits and Systems for Video Technology (IEEE TCSVT)</em>, 2017. &nbsp <font color="red">(‰∏≠ÁßëÈô¢Â§ßÁ±ª1Âå∫)</font> &nbsp
        <br>
        <a href="https://ieeexplore.ieee.org/abstract/document/7984895">ieeexplore</a>
        /
        <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76">impact factor</a>
        <p></p>
        <p>
          We propose a single image dehazing method based on a physical model and the brightness components of the image by using a multi-scale retinex with color restoration algorithm.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/dehazedcp.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231214010157">
          <papertitle>Single Image Dehazing with a Physical Model and Dark Channel Prior</papertitle>
        </a>
        <br>
        <strong>Jinbao Wang</strong>, Ning He, Lulu Zhang, Ke Lu 
        <br>
        <em>Neurocomputing</em>, 2015. &nbsp <font color="red">(ESI Highly-Cited Paper)</font>
        <br>
        <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231214010157">sciencedirect</a>
        /
        <a href="https://kd.nsfc.gov.cn/paperDownload/1000018804437.pdf">nsfc</a> 
        /
        <a href="https://www.sciencedirect.com/journal/neurocomputing">impact factor</a>
        <p></p>
        <p>
          We propose a single image dehazing method that is based on a physical model and the dark channel prior principle.
        </p>
      </td>
    </tr>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading><b> Publication List</b></heading>
        <p>Note that * contributed equally, ‚Ä† corresponding authors.</p>
        <p> <b> Conference </b></p>
        <ol>
          <li>
            Hanzhe Liang, Jie Zhang, Tao Dai, Linlin Shen, <strong>Jinbao Wang</strong>, and Can Gao. 
            "Taming Anomalies with Down-Up Sampling Networks: Group Center Preserving Reconstruction for 3D Anomaly Detection." 
            The 33st ACM International Conference on Multimedia (ACM MM). 2025. &nbsp <font color="red">(CCF-A)</font> &nbsp
          </li>
          <li>
            Dezhi Zheng, Kaijun Deng, Xianxu Hou, <strong>Jinbao Wang</strong>, Xiaoqin Wang, and Linlin Shen. 
            "Unknown Pixel Mask based finetuning of 2D Inpainting Models for Unbounded 3D Scene Generation from a Single Image." 
            The 33st ACM International Conference on Multimedia (ACM MM). 2025. &nbsp <font color="red">(CCF-A)</font> &nbsp
          </li>
          <li>
            Qingyuan Liu, Ke Lv, Kun Dong, Jian Xue, Zehai Niu, and <strong>Jinbao Wang</strong>. 
            "Text-to-Any-Skeleton Motion Generation Without Retargeting." 
            The IEEE/CVF International Conference on Computer Vision (ICCV). 2025. &nbsp <font color="red">(CCF-A)</font> &nbsp
          </li>
          <li>
            Yu Huang, Xiaowen Fu, Ling Ma, Zhengqi He, <strong>Jinbao Wang</strong>, and Xueliang Li. 
            "HilComp: Hilbert Curve-Based Balanced Clustering for 3D Gaussian Splatting Compression." 
            IEEE International Conference on Image Processing (ICIP). 2025.
          </li>
          <li>
            Jiayi Cheng, Can Gao, Jie Zhou, Jiajun Wen, Tao Dai, and <strong>Jinbao Wang</strong>. 
            "MC3D-AD: A Unified Geometry-aware Reconstruction Model for Multi-category 3D Anomaly Detection." 
            The 34th International Joint Conference on Artificial Intelligence (IJCAI). 2025. &nbsp <font color="red">(CCF-A)</font> &nbsp
          </li>
          <li>
            Kaijun Deng, Dezhi Zheng, Jindong Xie, <strong>Jinbao Wang</strong>, Weicheng Xie, Linlin Shen, and Siyang Song. 
            "DEGSTalk: Decomposed Per-Embedding Gaussian Fields for Hair-Preserving Talking Face Synthesis."  
            IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). 2025.
          </li>
          <li>
            Jindong Xie, Jiachen Liu, Yupei Lin, <strong>Jinbao Wang</strong>, Xianxu Hou, and Linlin Shen. 
            "High-Fidelity Editable Portrait Synthesis with 3D GAN Inversion."  
            IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). 2025.
          </li>
          <li>
            Dezhi Zheng, Kaijun Deng, <strong>Jinbao Wang</strong>, and Linlin Shen. 
            "Dual Encoders for Diffusion-based Image Inpainting."  
            IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). 2025.
          </li>
          <li>
            Biqiao Xin, Qiang Li, Qianchen Mao, <strong>Jinbao Wang</strong>, and Bingshu Wang. 
            "FBI-Net: Frequency Band Integration Network for Infrared Small Target Segmentation."  
            IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). 2025.
          </li>
          <li>
            Hanzhe Liang, Guoyang Xie, Chengbin Hou, Bingshu Wang, Can Gao‚Ä†, and <strong>Jinbao Wang</strong>‚Ä†. 
            "Look Inside for More: Internal Spatial Modality Perception for 3D Anomaly Detection."  
            The 39th AAAI Conference on Artificial Intelligence (AAAI). 2025. &nbsp <font color="red">(CCF-A)</font> &nbsp 
          </li>
          <li>
            Linchao Pan, Can Gao, Jie Zhou, and <strong>Jinbao Wang</strong>.
            "Learning with Open-world Noisy Data via Class-independent Margin in Dual Representation Space." 
            The 39th AAAI Conference on Artificial Intelligence (AAAI). 2025. &nbsp <font color="red">(CCF-A)</font> &nbsp
          </li>
          <li>
            Tao Dai, Yang Lin, Hang Guo, <strong>Jinbao Wang</strong>, and Zexuan Zhu. 
            "DCSF-KD: Dynamic Channel-wise Spatial Feature Knowledge Distillation for Object Detection." 
            The 39th AAAI Conference on Artificial Intelligence (AAAI). 2025. &nbsp <font color="red">(CCF-A)</font> &nbsp 
          </li>
          <li>
            Yu Zeng, Yang Zhang, Linlin Shen, Jiachen Liu, Kaijun Deng, Weizhao He, and <strong>Jinbao Wang</strong>. 
            "HairDiffusion: Vivid Multi-Colored Hair Editing via Latent Diffusion." 
            The 38th Conference on Neural Information Processing Systems (NeurIPS). 2024. &nbsp <font color="red">(CCF-A)</font> &nbsp 
          </li>
          <li>
            Zehai Niu, Ke Lv, Kun Dong, Jian Xue, Xiaoyu Qin, <strong>Jinbao Wang</strong>. 
            "Motion Reconstruction via Human Anatomy Diffusion from Sparse Tracking." 
            Computer Vision (ECCVW). 2024.
          </li>
          <li>
            Qingyuan Liu, Ke Lv, Zehai Niu, Kun Dong, <strong>Jinbao Wang</strong>, Jian Xue, and Xiaoyu Qin. 
            "FlexControl: Flexible and Efficient Full-Body Controllable Text-to-Motion Generation." 
            Towards a Complete Analysis of People: Fine-grained Understanding for Real-World Applications (ECCVW). 2024. 
          </li>
          <li>
            Qingyuan Liu, Ke Lv, Zehai Niu, Kun Dong, <strong>Jinbao Wang</strong>, Jian Xue, and Xiaoyu Qin. 
            "FlexControl: Flexible and Efficient Full-Body Controllable Text-to-Motion Generation." 
            Towards a Complete Analysis of People: Fine-grained Understanding for Real-World Applications (ECCVW). 2024. 
          </li>
          <li>
            Qingyuan Liu, Zehai Niu, Ke Lu, Kun Dong, Jian Xue, Xiaoyu Qin, and <strong>Jinbao Wang</strong>. 
            "AdaptControl: Adaptive Human Motion Control and Generation via User Prompt and Spatial Trajectory Guidance." 
            The 5th International Workshop on Human-centric Multimedia Analysis (ACM MMW). 2024. &nbsp <font color="red">(Best Student Paper)</font> &nbsp
          </li>
          <li>
            Hongze Zhu, Guoyang Xie, Chengbin Hou, Tao Dai, Can Gao, <strong>Jinbao Wang</strong>‚Ä†, and Linlin Shen. 
            "Towards High-resolution 3D Anomaly Detection via Group-Level Feature Contrastive Learning." 
            The 32st ACM International Conference on Multimedia (ACM MM). 2024. &nbsp <font color="red">(CCF-A)</font> &nbsp
          </li>
          <li>
            Xinpeng Li, Teng Wang, Shuyi Mao, <strong>Jinbao Wang</strong>, Jian Zhao, Xiaojiang Peng, Feng Zheng, and Xuelong Li. 
            "Two in One Go: Single-stage Emotion Recognition with Decoupled Subject-context Transformer." 
            The 32st ACM International Conference on Multimedia (ACM MM). 2024. &nbsp <font color="red">(CCF-A)</font> &nbsp
          </li>
          <li>
            Lian Chen, Zehai Niu, Qingyuan Liu, <strong>Jinbao Wang</strong>, Jian Xue, and Ke Lu. 
            "Anatomically-informed Vector Quantization Variational Auto-encoder for Text to Motion Generation." 
            IEEE International Conference on Multimedia and Expo Workshop (ICMEW). 2024.
          </li>
          <li>
            Tao Dai, Jianping Wang, Hang Guo, Jinmin Li, <strong>Jinbao Wang</strong>‚Ä†, and Zexuan Zhu‚Ä†. 
            "FreqFormer: Frequency-aware Transformer for Lightweight Image Super-resolution." 
            The 33rd International Joint Conference on Artificial Intelligence (IJCAI). 2024. &nbsp <font color="red">(CCF-A)</font> &nbsp
          </li>
          <li>
            Qiang Li, Qianchen Mao, Wenjie Liu, <strong>Jinbao Wang</strong>, Wenming Wang, and Binshu Wang. 
            "Local Information Guided Global Integration For Infrared Small Target Detection." 
            IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). 2024.
          </li>
          <li>
            Jiaqi Liu, Kai Wu, Qiang Nie, Ying Chen, Bin-Bin Gao, Yong Liu, <strong>Jinbao Wang</strong>, Chengjie Wang, and Feng Zheng. 
            "Unsupervised Continual Anomaly Detection with Contrastively-learned Prompt." 
            The 38th AAAI Conference on Artificial Intelligence (AAAI). 2024. &nbsp <font color="red">(CCF-A)</font> &nbsp
          </li>
          <li>
            Jiaqi Liu, Guoyang Xie, Ruitao Chen, Xinpeng Li, <strong>Jinbao Wang</strong>‚Ä†, Yong Liu, Chengjie Wang, and Feng Zheng‚Ä†. 
            "Real3D-AD: A Dataset of Point Cloud Anomaly Detection." 
            The 37th Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks. 2023. &nbsp <font color="red">(CCF-A)</font> &nbsp
          </li>
          <li>
            Ruitao Chen, Guoyang Xie, Jiaqi Liu, <strong>Jinbao Wang</strong>‚Ä†, Ziqi Luo, Jinfan Wang, Feng Zheng‚Ä†. 
            "EasyNet: An Easy Network for 3D Industrial Anomaly Detection." 
            The 31st ACM International Conference on Multimedia (ACM MM). 2023. &nbsp <font color="red">(CCF-A)</font> &nbsp
          </li>
          <li>
            Lingrui Zhang, Shuheng Zhang, Guoyang Xie, Jiaqi Liu, Hua Yan, <strong>Jinbao Wang</strong>‚Ä†, Feng Zheng‚Ä†, and Yaochu Jin. 
            "What Makes a Good Data Augmentation for Few-shot Unsupervised Image Anomaly Detection?" 
            The IEEE/CVF Conference on Computer Vision and Pattern Recognition Vision Workshop (CVPRW). 2023.
          </li>
          <li>
            Guoyang Xie*, <strong>Jinbao Wang</strong>*‚Ä†, Jiaqi Liu*, Feng Zheng‚Ä†, and Yaochu Jin. 
            "Pushing the Limits of Fewshot Anomaly Detection in Industry Vision: Graphcore."
            The 11th International Conference on Learning Representations (ICLR). 2023.
          </li>
          <li>
            Wujin Li, Jiawei Zhan, <strong>Jinbao Wang</strong>‚Ä†, Bizhong Xia, Bin-Bin Gao, Jun Liu, Chengjie Wang, and Feng Zheng‚Ä†. 
            "Towards Continual Adaptation in Industrial Anomaly Detection." 
            The 30th ACM International Conference on Multimedia (ACM MM). 2022. &nbsp <font color="red">(CCF-A)</font> &nbsp
          </li>
          <li>
            Xi Jiang, Jianlin Liu, <strong>Jinbao Wang</strong>‚Ä†, Qiang Nie, Kai Wu, Yong Liu, Chengjie Wang, and Feng Zheng‚Ä†. 
            "SoftPatch: Unsupervised Anomaly Detection with Noisy Data." 
            The 36th Conference on Neural Information Processing Systems (NeurIPS). 2022. &nbsp <font color="red">(CCF-A)</font> &nbsp
          </li>
          <li>
            <strong>Jinbao Wang</strong>*, Guoyang Xie*, Yawen Huang*, Yefeng Zheng, Yaochu Jin, and Feng Zheng. 
            "FedMed-ATL: Misaligned Unpaired Cross-modality Neuroimage Synthesis via Affine Transform Loss." 
            The 30th ACM International Conference on Multimedia (ACM MM). 2022. &nbsp <font color="red">(CCF-A)</font> &nbsp
          </li>
          <li>
            Hongjun Chen, <strong>Jinbao Wang</strong>, Hong Cai Chen, Xiantong Zhen, Feng Zheng, Rongrong Ji, and Ling Shao. 
            "Seminar Learning for Click-level Weakly Supervised Semantic Segmentation." 
            The IEEE/CVF International Conference on Computer Vision (ICCV). 2021. &nbsp <font color="red">(CCF-A)</font> &nbsp
          </li>
          <li>
            Lian Chen, Ke Lu, Pengcheng Gao, Jian Xue, and <strong>Jinbao Wang</strong>. 
            "A Novel Multi-feature Skeleton Representation for 3D Action Recognition." 
            International Conference on Pattern Recognition (ICPR). 2021.
          </li>
          <li>
            <strong>Jinbao Wang</strong>, Ke Lu, Jian Xue, and Yutong Kou. 
            "Relative Depth Estimation Prior for Single Image Dehazing." 
            IEEE International Conference on Multimedia & Expo Workshops (ICMEW). 2019.
          </li>
          <li>
            <strong>Jinbao Wang</strong>, Ke Lu, Jian Xue, Pengcheng Gao, and Yanfu Yan. 
            "A Markerless Body Motion Capture System for Character Animation Based on Multi-view Cameras." 
            IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2019.
          </li>
          <li>
            <strong>Jinbao Wang</strong>, Ning He, and Ke Lu. 
            "A New Single Image Dehazing Method with MSRCR Algorithm." 
            The 7th International Conference on Internet Multimedia Computing and Service (ICIMCS). 2015.
          </li>
          <li>
            Ning He, Ke Lu, and <strong>Jinbao Wang</strong>. 
            "Image Denoising Using Fractional-order Non-local TV Model." 
            International Conference on Internet Multimedia Computing and Service (ICIMCS). 2014.
          </li>
        </ol>

        <p><b>Journal</b></p>
        <ol>
          <li>
            <strong>Jinbao Wang</strong>, Jiayi Cheng, Can Gao, Jie Zhou, and Linlin Shen 
            "Enhanced Fabric Defect Detection with Feature Contrast Interference Suppression." 
            IEEE Transactions on Instrumentation and Measurement (IEEE TIM) 74 (2025): 1-12.
            <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=19">IF</a>
            &nbsp <font color="red">(‰∏≠ÁßëÈô¢Â§ßÁ±ª2Âå∫)</font> &nbsp
          </li>
          <li>
            Can Gao, Xiujian Chen, Jie Zhou, <strong>Jinbao Wang</strong>, and Linlin Shen. 
            "Open-Set Fabric Defect Detection with Defect Generation and Transfer." 
            IEEE Transactions on Instrumentation and Measurement (IEEE TIM) 74 (2025): 1-13.
            <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=19">IF</a>
            &nbsp <font color="red">(‰∏≠ÁßëÈô¢Â§ßÁ±ª2Âå∫)</font> &nbsp
          </li>
          <li>
            Jiayi Lyu, Xing Lan, Guohong Hu, Hanyu Jiang, Wei Gan, <strong>Jinbao Wang</strong>, and Jian Xue. 
            "Multimodal Emotional Talking Face Generation Based on Action Units." 
            IEEE Transactions on Circuits and Systems for Video Technology (IEEE TCSVT). 2024.
            <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76">IF</a>
            &nbsp <font color="red">(‰∏≠ÁßëÈô¢Â§ßÁ±ª1Âå∫)</font> &nbsp
          </li>
          <li>
            Bingshu Wang, Haosu Zhang, Qiang Li, Qianchen Mao, <strong>Jinbao Wang</strong>, C.L. Philip Chen, Aihong Shangguan, and Haosu Zhang. 
            "A Survey on Vision-Based Anti Unmanned Aerial Vehicles Methods." 
            Drones 8(9) 2024: 518.
            <a href="https://www.mdpi.com/journal/drones">IF</a>
          </li>            
          <li>
            Zehai Niu, Ke Lu, Jian Xue, Xiaoyu Qin, <strong>Jinbao Wang</strong>, and Ling Shao. 
            "From Methods to Applications: A Review of Deep 3D Human Motion Capture." 
            IEEE Transactions on Circuits and Systems for Video Technology (IEEE TCSVT) 34(11) 2024: 11340-11359.
            <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76">IF</a>
            &nbsp <font color="red">(‰∏≠ÁßëÈô¢Â§ßÁ±ª1Âå∫)</font> &nbsp
          </li>  
          <li>
            Zehai Niu, Ke Lu, Jian Xue, and <strong>Jinbao Wang</strong>. 
            "Skeleton Cluster Tracking for Robust Multi-view Multi-person 3D Human Pose Estimation." 
            Computer Vision and Image Understanding (CVIU) 246 (2024): 104059. 
            <a href="https://www.sciencedirect.com/journal/computer-vision-and-image-understanding">IF</a>
          </li>         
          <li>
            Guoyang Xie*, <strong>Jinbao Wang</strong>*, Jiaqi Liu*, Jiayi Lyu, Yong Liu, Chengjie Wang, Feng Zheng, and Yaochu Jin. 
            "IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing." 
            IEEE Transactions on Cybernetics (IEEE TCYB) 54(5) 2024: 2720-2733. 
            <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6221036">IF</a>
            &nbsp <font color="red">(‰∏≠ÁßëÈô¢Â§ßÁ±ª1Âå∫)</font> &nbsp
          </li>
          <li>
            Jiaqi Liu*, Guoyang Xie*, <strong>Jinbao Wang</strong>*, Shangnian Li, Chengjie Wang, Feng Zheng, and Yaochu Jin. 
            "Deep Industrial Image Anomaly Detection: A Survey." 
            Machine Intelligence Research (MIR) 21(1) 2024: 104-135. 
            <a href="https://www.springer.com/journal/11633">IF</a>
          </li>
          <li>
            Wujin Li, Bin-Bin Gao, Bizhong Xia, <strong>Jinbao Wang</strong>, Jun Liu, Yong Liu, Chengjie Wang, and Feng Zheng. 
            "Cross-Modal Alternating Learning with Task-Aware Representations for Continual Learning." 
            IEEE Transactions on Multimedia (IEEE TMM) 26 (2023): 5911-5924.
            <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">IF</a>
            &nbsp <font color="red">(‰∏≠ÁßëÈô¢Â§ßÁ±ª1Âå∫)</font> &nbsp
          </li>
          <li>
            Guoyang Xie*, Yawen Huang*, <strong>Jinbao Wang</strong>‚Ä†, Jiayi Lyu, Feng Zheng‚Ä†, Yefeng Zheng, and Yaochu Jin. 
            "Cross-modality Neuroimage Synthesis: A Survey." 
            ACM Computing Surveys 56 (2023): 1-28. 
            <a href="https://dl.acm.org/journal/csur">IF</a>
            &nbsp <font color="red">(‰∏≠ÁßëÈô¢Â§ßÁ±ª1Âå∫)</font> &nbsp
          </li>
          <li>
            Guoyang Xie*, <strong>Jinbao Wang</strong>*, Guo Yu, Feng Zheng, and Yaochu Jin. 
            "Tiny Adversarial Mulit-objective Oneshot Neural Architecture Search." 
            Complex & Intelligent Systems (CIS) 6 (2023): 107-109. 
            <a href="https://www.springer.com/journal/40747">IF</a>
          </li>
          <li>
            <strong>Jinbao Wang</strong>*, Xie, Guoyang*, Yawen Huang*, Jiayi Lyu, Feng Zheng, Yefeng Zheng, and Yaochu Jin. 
            "FedMed-GAN: Federated Domain Translation on Unsupervised Cross-modality Brain Image Synthesis." 
            Neurocomputing 546 (2023): 126282. 
            <a href="https://www.sciencedirect.com/journal/neurocomputing">IF</a>
          </li>
          <li>
            Hao Zheng*, <strong>Jinbao Wang</strong>*, Xiantong Zhen, Jingkuan Song, Feng Zheng, Ke Lu, and Guo-Jun Qi. 
            "Continuous Cross-modal Hashing." 
            Pattern Recognition (PR) 142 (2023): 109662. 
            <a href="https://www.sciencedirect.com/journal/pattern-recognition">IF</a>
            &nbsp <font color="red">(‰∏≠ÁßëÈô¢Â§ßÁ±ª1Âå∫)</font> &nbsp 
          </li>
          <li>
            <strong>Jinbao Wang</strong>, Shuo Xu, Feng Zheng, Ke Lu, Jingkuan Song, and Ling Shao. 
            "Learning Efficient Hash Codes for Fast Graph-based Data Similarity Retrieval. 
            "IEEE Transactions on Image Processing (IEEE TIP) 30 (2021): 6321-6334.
            <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">IF</a>
            &nbsp <font color="red">(CCF-A)</font> &nbsp &nbsp <font color="red">(‰∏≠ÁßëÈô¢Â§ßÁ±ª1Âå∫)</font> &nbsp
          </li>
          <li>
            <strong>Jinbao Wang</strong>*, Shujie Tan*, Xiantong Zhen, Shuo Xu, Feng Zheng, Zhenyu He, and Ling Shao. 
            "Deep 3D Human Pose Estimation: A Review." 
            Computer Vision and Image Understanding (CVIU) 210 (2021): 103225. 
            <a href="https://www.sciencedirect.com/journal/computer-vision-and-image-understanding">IF</a>
          </li>
          <li>
            <strong>Jinbao Wang</strong>, Ke Lu, Jian Xue, Ning He, and Ling Shao. 
            "Single Image Dehazing Based on the Physical Model and MSRCR Algorithm." 
            IEEE Transactions on Circuits and Systems for Video Technology (IEEE TCSVT) 28(9) 2017: 2190-2199. 
            <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76">IF</a>
            &nbsp <font color="red">(‰∏≠ÁßëÈô¢Â§ßÁ±ª1Âå∫)</font> &nbsp
          </li>
          <li>
            <strong>Jinbao Wang</strong>, Ning He, Lulu Zhang, and Ke Lu. 
            "Single Image Dehazing with a Physical Model and Dark Channel Prior." 
            Neurocomputing 149 (2015): 718-728.
            <a href="https://www.sciencedirect.com/journal/neurocomputing">IF</a>
            &nbsp <font color="red">(ESI Highly-Cited Paper)</font> &nbsp
          </li>
          <li>
            Ning He, <strong>Jinbao Wang</strong>, Lu-Lu Zhang, and Ke Lu. 
            "An Improved Fractional-order Differentiation Model for Image Denoising." 
            Signal Processing 112 (2015): 180-188. 
            <a href="https://www.sciencedirect.com/journal/signal-processing">IF</a>
          </li>
          <li>
            Ning He, <strong>Jinbao Wang</strong>, Lu-Lu Zhang, Guang-Mei Xu, and Ke Lu. 
            "Non-local Sparse Regularization Model with Application to Image Denoising." 
            Multimedia Tools and Applications 75(5) 2016: 2579-2594. 
            <a href="https://www.springer.com/journal/11042">IF</a>
          </li>
          <li>
            Ning He, <strong>Jinbao Wang</strong>, Lu-Lu Zhang, and Ke Lu. 
            "Convex Optimization Based Low-rank Matrix Decomposition for Image Restoration." 
            Neurocomputing 172 (2016): 253-261. 
            <a href="https://www.sciencedirect.com/journal/neurocomputing">IF</a>
          </li>
          <li>
            Ning He, Ke Lu, Bing-Kun Bao, Lu-Lu Zhang, and <strong>Jinbao Wang</strong>. 
            "Single-image Motion Deblurring Using an Adaptive Image Prior." 
            Information Sciences 281 (2014): 736-749. 
            <a href="https://www.springer.com/journal/11042">IF</a>
          </li>
        </ol>
      </td>
    </tr>
  </tbody></table>

  <table style="width:40%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>	
  </tbody></table>
  <table style="width:40%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td style="padding:0px">
      <!-- Draw maps -->
      <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=IUVUsbFoYCiF6ZA9DEHi0yzQv6joh_M3irMnuB8sZ40&cl=ffffff&w=a"></script>
      <!-- <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=IUVUsbFoYCiF6ZA9DEHi0yzQv6joh_M3irMnuB8sZ40"></script> -->
      </td>
    </tr>
  </tbody></table>
</td>
</tr>
</table>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>	
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. Thanks to <a href="https://jonbarron.info">JonBarron</a>. 
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>
</html>
