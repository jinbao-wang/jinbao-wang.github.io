<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jinbao Wang (ÁéãÈáëÂÆù)</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üìç</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>PhD Jinbao Wang</name>
              </p>
              <p>I am an <a href="https://bdsc.szu.edu.cn/home?p=1062&f=51&m=90&lan=zh&tag_id=100004">Assistant Professor</a> at <a href="https://bdsc.szu.edu.cn/home?p=1001&lan=zh">National Engineering Laboratory for Big Data System Computing Technology</a>, <a href="https://www.szu.edu.cn">Shenzhen University (SZU)</a>, where I work on digital human modelling and driving, image anomaly detection, computer vision and machine learning.
              </p>
              <p style="text-align:center">
                <a href="data/Jinbao_Wang_CV.pdf">CV</a> &nbsp/&nbsp
                <a href="data/Jinbao_Wang_CV_CH.pdf">CV(‰∏≠Êñá)</a> &nbsp/&nbsp
                <a href="data/Jinbao_Wang_Bio.txt">Bio</a> &nbsp/&nbsp
                <a href="https://github.com/jinbao-wang">Github</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=qI80ipUAAAAJ">Google Scholar</a> &nbsp&nbsp
                <!-- <a href="mailto:linkingring@163.com">linkingring@163.com</a>  -->
              </p>
              <p>
                If you have any questions or wish to cooperate with us, please feel free to contact me. 
                <a href="mailto:wangjb@szu.edu.cn">wangjb@szu.edu.cn</a> / <a href="mailto:linkingring@163.com">linkingring@163.com</a> / <a href="mailto:wangjb@ieee.org">wangjb@ieee.org</a>
              </p>
            </td>
            <td style="padding:2.5%;width:27%;max-width:27%">
              <a href="images/Jinbao-Wang.jpg"><img style="width:80%;max-width:80%" alt="profile photo" src="images/Jinbao-Wang.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading> <b>Research</b></heading>
              <p>
                I'm interested in image anomaly detection, graph representation learning, computer vision, and machine learning.
                Much of my research is about detecting and localizing anomalies from images. Representative papers are <span class="highlight">highlighted</span>.
              </p>
              <p>Note that *contributed equally, ‚Ä†corresponding author</p>
            </td>
          </tr>
        </tbody> </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

  <!-- Image Anomaly Detection -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
      <p><b> üî• &nbsp&nbsp&nbsp&nbsp&nbsp Image Anomaly Detection </b></p> 
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/imiad.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2301.13359.pdf">
          <papertitle>IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing</papertitle>
        </a>
        <br>
        Guoyang Xie*, <strong>Jinbao Wang</strong>*, Jiaqi Liu*, Jiayi Lyu, Yong Liu, Chengjie Wang, Feng Zheng, Yaochu Jin
        <br>
        <em>IEEE Transactions on Cybernetics (IEEE TCYB)</em>, 2024
        <br>
        <a href="https://github.com/M-3LAB/open-iad">project page</a>
        /
        <a href="https://arxiv.org/pdf/2301.13359.pdf">arXiv</a>
        /
        <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6221036">impact factor</a>
        <p></p>
        <p>
          We propose a large-scale systematic benchmark and uniform setting for IAD to bridge the gap between academy and industrial manufacturing. 
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/alta.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/document/10347466">
          <papertitle>Cross-Modal Alternating Learning with Task-Aware Representations for Continual Learning
            </papertitle>
        </a>
        <br>
        Wujin Li, Bin-Bin Gao, Bizhong Xia, <strong>Jinbao Wang</strong>, Jun Liu, Yong Liu, Chengjie Wang, and Feng Zheng
        <br>
        <em> IEEE Transactions on Multimedia (IEEE TMM) </em>, 2023
        <br>
        <a href="NeedUpload">project page</a>
        /
        <a href="https://ieeexplore.ieee.org/document/10347466">ieeexplore</a>
        /
        <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">impact factor</a>
        <p></p>
        <p>
          This paper proposes a novel yet effective framework coined cross-modal Alternating Learning with Task-Aware representations (ALTA) to make good use of visual and linguistic modal information and achieve more effective continual learning.
        </p>
      </td>
    </tr>
    
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/ucad.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="NeedUpload">
          <papertitle>Unsupervised Continual Anomaly Detection with Contrastively-learned Prompt</papertitle>
        </a>
        <br>
        Jiaqi Liu, Kai Wu, Qiang Nie, Ying Chen, Bin-Bin Gao, Yong Liu, <strong>Jinbao Wang</strong>, Chengjie Wang, Feng Zheng. 
        <br>
        <em> Association for the Advancement of Artificial Intelligence (AAAI) </em>, 2023
        <br>
        <a href="https://github.com/shirowalker/UCAD">project page</a>
        /
        <a href="NeedUpload">arxiv</a>
        <p></p>
        <p>
          This project introduces a novel Unsupervised Continual Anomaly Detection framework (UCAD), which equips the unsupervised AD with continual learning capability through contrastively-learned prompts.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src='images/real3dad.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2309.13226.pdf">
          <papertitle>Real3D-AD: A Dataset of Point Cloud Anomaly Detection</papertitle>
        </a>
        <br>
        Jiaqi Liu, Guoyang Xie, Ruitao Chen, Xinpeng Li, <strong>Jinbao Wang</strong>‚Ä†, Yong Liu, Chengjie Wang, Feng Zheng‚Ä†
        <br>
        <em> NeurIPS Datasets & Benchmarks Track </em>, 2023 &nbsp <font color="red">(CCF-A)</font> &nbsp
        <br>
        <a href="https://github.com/M-3LAB/Real3D-AD">project page</a>
        /
        <a href="https://arxiv.org/pdf/2309.13226.pdf">openreview</a>
        <p></p>
        <p>
          This project aims to construct a new dataset of high-resolution 3D point clouds for anomaly detection tasks in real-world scenes. Real3D-AD comprises a total of 1,254 samples that are distributed across 12 distinct categories.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/easynet.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2307.13925.pdf">
          <papertitle>EasyNet: An Easy Network for 3D Industrial Anomaly Detection</papertitle>
        </a>
        <br>
        Ruitao Chen, Guoyang Xie, Jiaqi Liu, <strong>Jinbao Wang</strong>‚Ä†, Ziqi Luo, Jinfan Wang, Feng Zheng‚Ä†
        <br>
        <em>ACM Multimedia (ACM MM)</em>, 2023 &nbsp <font color="red">(CCF-A)</font> &nbsp
        <br>
        <a href="">project page</a>
        /
        <a href="https://arxiv.org/pdf/2307.13925.pdf">arXiv</a>
        <p></p>
        <p>
          This paper addresses a promising and challenging task, i.e., deployment-friendly 3D-AD and proposes an easy but effective neural network (EasyNet) to achieve competitive performance without using large pre-trained models and memory banks.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/graphcore.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2301.12082.pdf">
          <papertitle>Pushing the Limits of Fewshot Anomaly Detection in Industry Vision: Graphcore</papertitle>
        </a>
        <br>
        Guoyang Xie*, <strong>Jinbao Wang</strong>*‚Ä†, Jiaqi Liu*, Feng Zheng‚Ä†, Yaochu Jin
        <br>
        <em>International Conference on Learning Representations (ICLR)</em>, 2023
        <br>
        <a href="https://github.com/M-3LAB/open-iad">project page</a>
        /
        <a href="https://openreview.net/pdf?id=xzmqxHdZAwO">openreview</a>
        /
        <a href="https://arxiv.org/pdf/2301.12082.pdf">arXiv</a>
        <p></p>
        <p>
          We reveal that rotation-invariant feature property has a significant impact in industrial-based fewshot anomaly detection.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/iadsurvey.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2301.11514.pdf">
          <papertitle>Deep Industrial Image Anomaly Detection: A Survey</papertitle>
        </a>
        <br>
        Jiaqi Liu*, Guoyang Xie*, <strong>Jinbao Wang</strong>*, Shangnian Li, Chengjie Wang, Feng Zheng, Yaochu Jin
        <br>
        <em>Machine Intelligence Research (MIR)</em>, 2023
        <br>
        <a href="https://github.com/M-3LAB/awesome-industrial-anomaly-detection">project page</a>
        /
        <a href="https://arxiv.org/pdf/2301.11514.pdf">arXiv</a>
        <p></p>
        <p>
          We provide a comprehensive review of deep learning-based IAD from the perspectives of neural network architectures, levels of supervision, loss functions, metrics and datasets. 
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/cvprw.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2304.03294.pdf">
          <papertitle>What Makes a Good Data Augmentation for Few-Shot Unsupervised Image Anomaly Detection</papertitle>
        </a>
        <br>
        Lingrui Zhang, Shuheng Zhang, Guoyang Xie, Jiaqi Liu, Hua Yan, <strong>Jinbao Wang</strong>‚Ä†, Feng Zheng‚Ä†, Yaochu Jin
        <br>
        <em>CVPR VISION Workshop (CVPRW)</em>, 2023
        <br>
        <a href="https://openaccess.thecvf.com/content/CVPR2023W/VISION/papers/Zhang_What_Makes_a_Good_Data_Augmentation_for_Few-Shot_Unsupervised_Image_CVPRW_2023_paper.pdf">openaccess</a>
        /
        <a href="https://arxiv.org/pdf/2304.03294.pdf">arXiv</a>
        <p></p>
        <p>
          We systematically investigate various data augmentation methods for few-shot IAD algorithms.  
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/softpatch.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://openreview.net/pdf?id=pIYYJflkhZ">
          <papertitle>SoftPatch: Unsupervised Anomaly Detection with Noisy Data</papertitle>
        </a>
        <br>
        Xi Jiang, Jianlin Liu, <strong>Jinbao Wang</strong>‚Ä†, Qiang Nie, W. U. Kai, Yong Liu, Chengjie Wang, Feng Zheng‚Ä†
        <br>
        <em>Neural Information Processing Systems (NeurIPS)</em>, 2022 &nbsp <font color="red">(CCF-A)</font> &nbsp
        <br>
        <a href="https://github.com/M-3LAB/open-iad">project page</a>
        /
        <a href="https://openreview.net/pdf?id=pIYYJflkhZ">openreview</a>
        <p></p>
        <p>
          We propose a memory-based unsupervised AD method, which efficiently denoises the data at the patch level.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/continualad.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2201.12589.pdf">
          <papertitle>Towards Continual Adaptation in Industrial Anomaly Detection</papertitle>
        </a>
        <br>
        Wujin Li, Jiawei Zhan, <strong>Jinbao Wang</strong>‚Ä†, Bizhong Xia, Bin-Bin Gao, Jun Liu, Chengjie Wang, Feng Zheng‚Ä†
        <br>
        <em>ACM Multimedia (ACM MM)</em>, 2022 &nbsp <font color="red">(CCF-A)</font> &nbsp
        <br>
        <a href="https://github.com/M-3LAB/open-iad">project page</a>
        /
        <a href="https://openreview.net/pdf?id=pIYYJflkhZ">openreview</a>
        <p></p>
        <p>
          We propose a unified framework by incorporating continual learning to achieve our newly designed task of continual anomaly detection.
        </p>
      </td>
    </tr>

  <!-- Medical Federated Learning -->
    <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <p><b> üëë &nbsp&nbsp&nbsp&nbsp&nbsp Medical Federated Learning </b></p> 
    </td>
    </tr>

    <!-- <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/kcross.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="">
          <papertitle>K-Space-Aware Cross-Modality Score for Quality Assessment of Synthesized Neuroimages</papertitle>
        </a>
        <br>
        <strong>Jinbao Wang</strong>*, Guoyang Xie*, Yawen Huang*, Jiayi Lyu, Feng Zheng, Yefeng Zheng, Yaochu Jin
        <br>
        <em>IEEE Journal of Biomedical and Health Informatics (IEEE JBHI) (Under Review)</em>, 2023
        <br>
        <a href="">arXiv</a>
        /
        <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6221020">impact factor</a>
        <p></p>
        <p>
          This paper proposes a novel metric, K-CROSS, for evaluating the performance of synthesized medical images based on the principles of magnetic resonance imaging. To enhance the reconstruction capability during K-CROSS training, we developed a complex U-Net. To train a learning-based full IQA metric, we constructed a large-scale multi-modal neuroimaging perceptual similarity (NIRPS) dataset.
        </p>
      </td>
    </tr> -->

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/medgansurvey.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2202.06997.pdf">
          <papertitle>Cross-Modality Neuroimage Synthesis: A Survey</papertitle>
        </a>
        <br>
        Guoyang Xie*, Yawen Huang*, <strong>Jinbao Wang</strong>‚Ä†, Jiayi Lyu, Feng Zheng‚Ä†, Yaochu Jin
        <br>
        <em>ACM Computing Surveys</em>, 2023
        <br>
        <a href="https://dl.acm.org/doi/abs/10.1145/3625227">acmdl</a>
        /
        <a href="https://arxiv.org/pdf/2202.06997.pdf">arXiv</a>
        /
        <a href="https://dl.acm.org/journal/csur">impact factor</a>
        <p></p>
        <p>
          We provide a comprehensive review of cross-modality synthesis for neuroimages, from the perspectives of weakly-supervised and unsupervised settings, loss functions, evaluation metrics, ranges of modality, datasets, and the synthesis-based downstream applications.     
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/fedmedgan.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231223004058?via%3Dihub">
          <papertitle>FedMed-GAN: Misaligned Unpaired Brain Image Synthesis via Transform Loss</papertitle>
        </a>
        <br>
        <strong>Jinbao Wang</strong>*, Guoyang Xie*, Yawen Huang*, Jiayi Lyu, Feng Zheng, Yefeng Zheng, Yaochu Jin
        <br>
        <em>Neurocomputing</em>, 2023
        <br>
        <a href="https://github.com/M-3LAB/FedMed-GAN">project page</a>
        /
        <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231223004058?via%3Dihub">sciencedirect</a>
        /
        <a href="https://arxiv.org/pdf/2201.08953.pdf">arXiv</a>
        /
        <a href="https://www.sciencedirect.com/journal/neurocomputing">impact factor</a>
        <p></p>
        <p>
          We propose a new benchmark for federated domain translation on unsupervised brain image synthesis to bridge the gap between federated learning and medical GAN.     
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/fedmedatl.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2201.12589.pdf">
          <papertitle>FedMed-ATL: Misaligned Unpaired Brain Image Synthesis via Transform Loss</papertitle>
        </a>
        <br>
        <strong>Jinbao Wang</strong>*, Guoyang Xie*, Yawen Huang*, Yefeng Zheng, Yaochu Jin, Feng Zheng
        <br>
        <em>ACM Multimedia (ACM MM)</em>, 2022 &nbsp <font color="red">(CCF-A)</font> &nbsp
        <br>
        <a href="https://github.com/M-3LAB/FedMed-GAN">project page</a>
        /
        <a href="https://arxiv.org/pdf/2201.12589.pdf">arXiv</a>
        <p></p>
        <p>
          We propose a method that can reduce the demands for deformable registration while encourage to leverage the misaligned and unpaired data. 
        </p>
      </td>
    </tr>

  <!-- Fast Retrieval -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
      <p><b> üê∞ &nbsp&nbsp&nbsp&nbsp&nbsp Fast Retrieval </b></p> 
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/ccmh.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320323003631">
          <papertitle>Continuous Cross-Modal Hashing</papertitle>
        </a>
        <br>
        Hao Zheng*, <strong>Jinbao Wang</strong>*, Jinbao Wang, Xiantong Zhen, Jingkuan Song, Feng Zheng, Ke Lu, Guo-Jun Qi
        <br>
        <em> Pattern Recognition (PR) </em>, 2023
        <br>
        <a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320323003631">sciencedirect</a>
        /
        <a href="https://www.sciencedirect.com/journal/pattern-recognition">imapct factor</a> 
        <p></p>
        <p>
          We propose a novel framework for the new task of continuous cross-modal hashing.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/tamnas.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2103.00363.pdf">
          <papertitle>Tiny Adversarial Multi-Objective Oneshot Neural Architecture Search</papertitle>
        </a>
        <br>
        Guoyang Xie*, <strong>Jinbao Wang</strong>*, Guo Yu, Jiayi Lyu, Feng Zheng, Yaochu Jin 
        <br>
        <em>Complex & Intelligent System (CIS)</em>, 2023
        <br>
        <a href="https://link.springer.com/article/10.1007/s40747-023-01139-8">springer</a>
        /
        <a href="https://www.springer.com/journal/40747">impact factor</a>
        <p></p>
        <p>
          We propose a multi-objective oneshot network architecture search algorithm to obtain the best trade-off networks in terms of the adversarial accuracy, the clean accuracy and the model size.      
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/hgnn.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/9474952">
          <papertitle>Learning Efficient Hash Codes for Fast Graph-Based Data Similarity Retrieval</papertitle>
        </a>
        <br>
        <strong>Jinbao Wang</strong>, Shuo Xu, Feng Zheng, Ke Lu, Jingkuan Song, and Ling Shao 
        <br>
        <em>IEEE Transactions on Image Processing (IEEE TIP) </em>, 2021 &nbsp <font color="red">(CCF-A)</font> &nbsp
        <br>
        <a href="https://ieeexplore.ieee.org/abstract/document/9474952">ieeexplore</a>
        /
        <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">impact factor</a>
        <p></p>
        <p>
          To represent graph-based data, and maintain fast retrieval while doing so, we introduce an efficient hash model with graph neural networks for fast graph-based data retrieval.
        </p>
      </td>
    </tr>

  <!-- Digital Human -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
      <p><b> üóø &nbsp&nbsp&nbsp&nbsp&nbsp Digital Human </b></p> 
      </td>
    </tr> 

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/posesurvey.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://www.sciencedirect.com/science/article/pii/S1077314221000692">
          <papertitle>Deep 3D Human Pose Estimation: A Review</papertitle>
        </a>
        <br>
        <strong>Jinbao Wang</strong>*, Shujie Tan*, Xiantong Zhen, Shuo Xu, Feng Zheng, Zhenyu He, and Ling Shao
        <br>
        <em>Computer Vision and Image Understanding (CVIU)</em>, 2021
        <br>
        <a href="https://www.sciencedirect.com/science/article/pii/S1077314221000692">sciencedirect</a>
        /
        <a href="https://www.sciencedirect.com/journal/computer-vision-and-image-understanding">impact factor</a>
        <p></p>
        <p>
          The recent methods for deep 3D pose estimation are categorized and thoroughly analyzed. Provide an extensive review of related datasets and evaluation metrics. Compare the pros and cons of the deep 3D models valuated on the datasets and draw a conclusion.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/motioncapture.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/8683506">
          <papertitle>A Markerless Body Motion Capture System for Character Animation Based on Multi-view Cameras</papertitle>
        </a>
        <br>
        <strong>Jinbao Wang</strong>, Ke Lu, Jian Xue, Pengcheng Gao, Yanfu Yan
        <br>
        <em>IEEE International Conference on Acoustics, Speech and Signal Processing (IEEE ICASSP)</em>, 2019
        <br>
        <a href="https://github.com/jinbao-wang/skinmesh">project page</a>
        /
        <a href="https://ieeexplore.ieee.org/abstract/document/8683506">ieeexplore</a>
        /
        <a href="https://arxiv.org/abs/2212.05788">arXiv</a> 
        <p></p>
        <p>
          A novel application system is proposed to achieve the generation of 3D character animation driven by markerless human body motion capture.
        </p>
      </td>
    </tr>

  <!-- image propcessing -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
      <p><b> ‚òòÔ∏è &nbsp&nbsp&nbsp&nbsp&nbsp Image Propcessing </b></p> 
      </td>
    </tr>  

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/clickseg.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Chen_Seminar_Learning_for_Click-Level_Weakly_Supervised_Semantic_Segmentation_ICCV_2021_paper.html">
          <papertitle>Seminar Learning for Click-Level Weakly Supervised Semantic Segmentation</papertitle>
        </a>
        <br>
        Hongjun Chen, <strong>Jinbao Wang</strong>, Hong Cai Chen, Xiantong Zhen, Feng Zheng, Rongrong Ji, Ling Shao
        <br>
        <em>IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 2021 &nbsp <font color="red">(CCF-A)</font> &nbsp
        <br>
        <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Seminar_Learning_for_Click-Level_Weakly_Supervised_Semantic_Segmentation_ICCV_2021_paper.pdf">openaccess</a>
        /
        <a href="https://arxiv.org/abs/2108.13393">arXiv</a> 
        <p></p>
        <p>
          We propose seminar learning, a new learning paradigm for semantic segmentation with click-level supervision.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/dehazemsrcr.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/7984895">
          <papertitle>Single Image Dehazing Based on the Physical Model and MSRCR Algorithm</papertitle>
        </a>
        <br>
        <strong>Jinbao Wang</strong>, Ke Lu, Jian Xue, Ning He, Ling Shao
        <br>
        <em>IEEE Transactions on Circuits and Systems for Video Technology (IEEE TCSVT)</em>, 2017
        <br>
        <a href="https://ieeexplore.ieee.org/abstract/document/7984895">ieeexplore</a>
        /
        <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76">impact factor</a>
        <p></p>
        <p>
          We propose a single image dehazing method based on a physical model and the brightness components of the image by using a multi-scale retinex with color restoration algorithm.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/dehazedcp.png' width="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231214010157">
          <papertitle>Single Image Dehazing with a Physical Model and Dark Channel Prior</papertitle>
        </a>
        <br>
        <strong>Jinbao Wang</strong>, Ning He, Lulu Zhang, Ke Lu 
        <br>
        <em>Neurocomputing</em>, 2015 &nbsp <font color="red">(Highly Cited Paper)</font>
        <br>
        <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231214010157">sciencedirect</a>
        /
        <a href="https://kd.nsfc.gov.cn/paperDownload/1000018804437.pdf">nsfc</a> 
        /
        <a href="https://www.sciencedirect.com/journal/neurocomputing">impact factor</a>
        <p></p>
        <p>
          We propose a single image dehazing method that is based on a physical model and the dark channel prior principle.
        </p>
      </td>
    </tr>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading><b> Publication List</b></heading>
        <p> <b> Conference </b></p>
        <ol>
          <li>
            Jiaqi Liu, Kai Wu, Qiang Nie, Ying Chen, Bin-Bin Gao, Yong Liu, <strong>Jinbao Wang</strong>, Chengjie Wang, and Feng Zheng. 
            "Unsupervised Continual Anomaly Detection with Contrastively-learned Prompt." 
            Association for the Advancement of Artificial Intelligence (AAAI). 2023.
          </li>
          <li>
            Jiaqi Liu, Guoyang Xie, Ruitao Chen, Xinpeng Li, <strong>Jinbao Wang</strong>‚Ä†, Yong Liu, Chengjie Wang, and Feng Zheng‚Ä†. 
            "Real3D-AD: A Dataset of Point Cloud Anomaly Detection." 
            Datasets & Benchmarks Track, Neural Information Processing Systems (NeurIPS). 2023. &nbsp <font color="red">(CCF-A)</font> &nbsp
          </li>
          <li>
            Ruitao Chen, Guoyang Xie, Jiaqi Liu, <strong>Jinbao Wang</strong>‚Ä†, Ziqi Luo, Jinfan Wang, Feng Zheng‚Ä†. 
            "EasyNet: An Easy Network for 3D Industrial Anomaly Detection." 
            In Proceedings of the 31st ACM International Conference on Multimedia (ACM MM). 2023. &nbsp <font color="red">(CCF-A)</font> &nbsp
          </li>
          <li>
            Lingrui Zhang, Shuheng Zhang, Guoyang Xie, Jiaqi Liu, Hua Yan, <strong>Jinbao Wang</strong>‚Ä†, Feng Zheng‚Ä†, and Yaochu Jin. 
            "What makes a good data augmentation for few-shot unsupervised image anomaly detection?" 
            In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR Vision Workshop), pp. 4344-4353. 2023.
          </li>
          <li>
            Guoyang Xie*, <strong>Jinbao Wang</strong>*‚Ä†, Jiaqi Liu*, Feng Zheng‚Ä†, and Yaochu Jin. 
            "Pushing the limits of fewshot anomaly detection in industry vision: Graphcore."
            The Eleventh International Conference on Learning Representations (ICLR). 2023.
          </li>
          <li>
            Wujin Li, Jiawei Zhan, <strong>Jinbao Wang</strong>‚Ä†, Bizhong Xia, Bin-Bin Gao, Jun Liu, Chengjie Wang, and Feng Zheng‚Ä†. 
            "Towards continual adaptation in industrial anomaly detection." 
            In Proceedings of the 30th ACM International Conference on Multimedia (ACM MM), pp. 2871-2880. 2022. &nbsp <font color="red">(CCF-A)</font> &nbsp
          </li>
          <li>
            Xi Jiang, Jianlin Liu, <strong>Jinbao Wang</strong>‚Ä†, Qiang Nie, Kai Wu, Yong Liu, Chengjie Wang, and Feng Zheng‚Ä†. 
            "SoftPatch: Unsupervised anomaly detection with noisy data." 
            Advances in Neural Information Processing Systems (NeurIPS), 35, pp. 15433-15445. 2022. &nbsp <font color="red">(CCF-A)</font> &nbsp
          </li>
          <li>
            <strong>Jinbao Wang</strong>*, Guoyang Xie*, Yawen Huang*, Yefeng Zheng, Yaochu Jin, and Feng Zheng. 
            "FedMed-ATL: Misaligned unpaired cross-modality neuroimage synthesis via affine transform loss." 
            In Proceedings of the 30th ACM International Conference on Multimedia (ACM MM), pp. 1522-1531. 2022. &nbsp <font color="red">(CCF-A)</font> &nbsp
          </li>
          <li>
            Hongjun Chen, <strong>Jinbao Wang</strong>, Hong Cai Chen, Xiantong Zhen, Feng Zheng, Rongrong Ji, and Ling Shao. 
            "Seminar learning for click-level weakly supervised semantic segmentation." 
            In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 6920-6929. 2021. &nbsp <font color="red">(CCF-A)</font> &nbsp
          </li>
          <li>
            Lian Chen, Ke Lu, Pengcheng Gao, Jian Xue, and <strong>Jinbao Wang</strong>. 
            "A novel multi-feature skeleton representation for 3d action recognition." 
            In International Conference on Pattern Recognition (ICPR), pp. 365-379. Springer, Cham, 2021.
          </li>
          <li>
            <strong>Jinbao Wang</strong>, Ke Lu, Jian Xue, and Yutong Kou. 
            "Relative depth estimation prior for single image dehazing." 
            In 2019 IEEE International Conference on Multimedia & Expo Workshops (ICMEW), pp. 270-275. IEEE, 2019.
          </li>
          <li>
            <strong>Jinbao Wang</strong>, Ke Lu, Jian Xue, Pengcheng Gao, and Yanfu Yan. 
            "A markerless body motion capture system for character animation based on multi-view cameras." 
            In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 8558-8562. IEEE, 2019.
          </li>
          <li>
            <strong>Jinbao Wang</strong>, Ning He, and Ke Lu. 
            "A new single image dehazing method with MSRCR algorithm." 
            In Proceedings of the 7th International Conference on Internet Multimedia Computing and Service, pp. 1-4. 2015.
          </li>
          <li>
            Ning He, Ke Lu, and <strong>Jinbao Wang</strong>. 
            "Image denoising using fractional-order non-local TV model." 
            In Proceedings of International Conference on Internet Multimedia Computing and Service, pp. 279-282. 2014.
          </li>
        </ol>

        <p><b>Journal</b></p>
        <ol>
          <!-- <li>
            <strong>Jinbao Wang</strong>*, Guoyang Xie*, Yawen Huang*, Jiayi Lyu, Feng Zheng, Yefeng Zheng, and Yaochu Jin. 
            "K-space-aware cross-modality score for synthesized neuroimage quality assessment." 
            IEEE Journal of Biomedical and Health Informatics (IEEE JBHI) (Under Review). 2023. 
            <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6221020">IF</a>
          </li> -->
          <li>
            Guoyang Xie*, <strong>Jinbao Wang</strong>*, Jiaqi Liu*, Jiayi Lyu, Yong Liu, Chengjie Wang, Feng Zheng, and Yaochu Jin. 
            "IM-IAD: Industrial image anomaly detection benchmark in manufacturing." 
            IEEE Transactions on Cybernetics (IEEE TCYB). 2024. 
            <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6221036">IF</a>
          </li>
          <li>
            Wujin Li, Bin-Bin Gao, Bizhong Xia, <strong>Jinbao Wang</strong>, Jun Liu, Yong Liu, Chengjie Wang, and Feng Zheng. 
            "Cross-Modal Alternating Learning with Task-Aware Representations for Continual Learning." 
            IEEE Transactions on Multimedia (IEEE TMM). 2023.
            <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">IF</a>
          </li>
          <li>
            Guoyang Xie*, Yawen Huang*, <strong>Jinbao Wang</strong>‚Ä†, Jiayi Lyu, Feng Zheng‚Ä†, Yefeng Zheng, and Yaochu Jin. 
            "Cross-modality neuroimage synthesis: A survey." 
            ACM Computing Surveys 56 (2023): 1-28. 
            <a href="https://dl.acm.org/journal/csur">IF</a>
          </li>
          <li>
            Jiaqi Liu*, Guoyang Xie*, <strong>Jinbao Wang</strong>*, Shangnian Li, Chengjie Wang, Feng Zheng, and Yaochu Jin. 
            "Deep industrial image anomaly detection: A survey." 
            Machine Intelligence Research (MIR). 2023. 
            <a href="https://www.springer.com/journal/11633">IF</a>
          </li>
          <li>
            Guoyang Xie*, <strong>Jinbao Wang</strong>*, Guo Yu, Feng Zheng, and Yaochu Jin. 
            "Tiny adversarial mulit-objective oneshot neural architecture search." 
            Complex & Intelligent Systems (CIS) 6 (2023): 107-109. 
            <a href="https://www.springer.com/journal/40747">IF</a>
          </li>
          <li>
            <strong>Jinbao Wang</strong>*, Xie, Guoyang*, Yawen Huang*, Jiayi Lyu, Feng Zheng, Yefeng Zheng, and Yaochu Jin. 
            "FedMed-GAN: Federated domain translation on unsupervised cross-modality brain image synthesis." 
            Neurocomputing 546 (2023): 126282. 
            <a href="https://www.sciencedirect.com/journal/neurocomputing">IF</a>
          </li>
          <li>
            Hao Zheng*, <strong>Jinbao Wang</strong>*, Xiantong Zhen, Jingkuan Song, Feng Zheng, Ke Lu, and Guo-Jun Qi. 
            "Continuous cross-modal hashing." 
            Pattern Recognition (PR) 142 (2023): 109662. 
            <a href="https://www.sciencedirect.com/journal/pattern-recognition">IF</a>
          </li>
          <li>
            <strong>Jinbao Wang</strong>, Shuo Xu, Feng Zheng, Ke Lu, Jingkuan Song, and Ling Shao. 
            "Learning efficient hash codes for fast graph-based data similarity retrieval. 
            "IEEE Transactions on Image Processing (IEEE TIP) 30 (2021): 6321-6334.
            <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">IF</a>
            &nbsp <font color="red">(CCF-A)</font> &nbsp
          </li>
          <li>
            <strong>Jinbao Wang</strong>*, Shujie Tan*, Xiantong Zhen, Shuo Xu, Feng Zheng, Zhenyu He, and Ling Shao. 
            "Deep 3D human pose estimation: A review." 
            Computer Vision and Image Understanding (CVIU) 210 (2021): 103225. 
            <a href="https://www.sciencedirect.com/journal/computer-vision-and-image-understanding">IF</a>
          </li>
          <li>
            <strong>Jinbao Wang</strong>, Ke Lu, Jian Xue, Ning He, and Ling Shao. 
            "Single image dehazing based on the physical model and MSRCR algorithm." 
            IEEE Transactions on Circuits and Systems for Video Technology (IEEE TCSVT) 28, no. 9 (2017): 2190-2199. 
            <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76">IF</a>
          </li>
          <li>
            <strong>Jinbao Wang</strong>, Ning He, Lulu Zhang, and Ke Lu. 
            "Single image dehazing with a physical model and dark channel prior." 
            Neurocomputing 149 (2015): 718-728.
            <a href="https://www.sciencedirect.com/journal/neurocomputing">IF</a>
            &nbsp <font color="red">(Highly Cited Paper)</font> &nbsp
          </li>
          <li>
            Ning He, <strong>Jinbao Wang</strong>, Lu-Lu Zhang, and Ke Lu. 
            "An improved fractional-order differentiation model for image denoising." 
            Signal Processing 112 (2015): 180-188. 
            <a href="https://www.sciencedirect.com/journal/signal-processing">IF</a>
          </li>
          <li>
            Ning He, <strong>Jinbao Wang</strong>, Lu-Lu Zhang, Guang-Mei Xu, and Ke Lu. 
            "Non-local sparse regularization model with application to image denoising." 
            Multimedia Tools and Applications 75, no. 5 (2016): 2579-2594. 
            <a href="https://www.springer.com/journal/11042">IF</a>
          </li>
          <li>
            Ning He, <strong>Jinbao Wang</strong>, Lu-Lu Zhang, and Ke Lu. 
            "Convex optimization based low-rank matrix decomposition for image restoration." 
            Neurocomputing 172 (2016): 253-261. 
            <a href="https://www.sciencedirect.com/journal/neurocomputing">IF</a>
          </li>
          <li>
            Ning He, Ke Lu, Bing-Kun Bao, Lu-Lu Zhang, and <strong>Jinbao Wang</strong>. 
            "Single-image motion deblurring using an adaptive image prior." 
            Information Sciences 281 (2014): 736-749. 
            <a href="https://www.springer.com/journal/11042">IF</a>
          </li>
        </ol>
      </td>
    </tr>
  </tbody></table>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>	
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. Thanks to <a href="https://jonbarron.info">JonBarron</a>. 
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
